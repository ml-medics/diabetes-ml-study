{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "This script is for training a multi-class classification model CNN from the given dataset using Keras + Tensorflow stack.\n",
    "1. Configure the paramers as required\n",
    "2. Configure the model as required\n",
    "3. Run all the cells (cell > Run all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "imports the required modules from keras, Tensorflow, PIL, numpy and custom file handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras import optimizers, regularizers, models, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from random import shuffle, randint\n",
    "import numpy as np\n",
    "from keras.applications import VGG16\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir, mkdir\n",
    "from os.path import join, isfile, isdir, exists, expanduser, dirname, realpath\n",
    "from shutil import rmtree, copyfile\n",
    "from keras.models import load_model, model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir_helper(dir_path):\n",
    "    if not exists(dir_path):\n",
    "        mkdir(dir_path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def delete_dir_helper(dir_path):\n",
    "    if exists(dir_path):\n",
    "        rmtree(dir_path)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def delete_dir_files_helper(dir_path):\n",
    "    for file in listdir(dir_path):\n",
    "        file_path = join(dir_path, file)\n",
    "        try:\n",
    "            if isfile(file_path):\n",
    "                unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def is_jpeg(fname):\n",
    "    return fname.split('.')[-1] == 'jpeg'\n",
    "\n",
    "\n",
    "def copy_file(src_dir, dst_dir):\n",
    "    copyfile(src_dir, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurable parameters\n",
    "Defines various constants, directory paths and design of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "project_dir_path = join(expanduser('~'),'Documents','RIT', 'Big Data Analysis', 'diabetes-ml-study');\n",
    "original_dataset_dir = join(project_dir_path, 'datasets', 'eye data');\n",
    "base_dir =  join(project_dir_path, 'datasets' , 'eye data', 'temp data');\n",
    "model_dir =  join(project_dir_path,'datasets' , 'models');\n",
    "\n",
    "MODEL_FILE_NAME = 'dr_model.h5'\n",
    "\n",
    "CLASSES_TO_CLASSIFY = ['0', '1', '2', '3', '4']\n",
    "print(CLASSES_TO_CLASSIFY)\n",
    "NO_OF_CLASSES = len(CLASSES_TO_CLASSIFY)\n",
    "\n",
    "ext = '.jpeg'\n",
    "IMAGE_SIZE = 128\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 15\n",
    "STEPS_PER_EPOCH = 5\n",
    "\n",
    "# Define the model\n",
    "def configure_model():\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    conv_base = VGG16(weights='imagenet' ,include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    for layer in conv_base.layers:\n",
    "        if 'block_5' in layer.name:\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "    \n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(NO_OF_CLASSES, activation='softmax'))\n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make required directories\n",
    "Make the directories for training data, validation data & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the previous data folder and all its contents\n",
    "# delete_dir_helper(base_dir)\n",
    "\n",
    "# Define train, test and validation directories\n",
    "train_dir = join(base_dir, 'train')\n",
    "validation_dir = join(base_dir, 'validation')\n",
    "test_dir = join(base_dir, 'test')\n",
    "model_file_path = join(model_dir, MODEL_FILE_NAME)\n",
    "\n",
    "make_dir_helper(base_dir)\n",
    "make_dir_helper(model_dir)\n",
    "\n",
    "# Make base directories\n",
    "make_dir_helper(train_dir)\n",
    "make_dir_helper(validation_dir)\n",
    "make_dir_helper(test_dir)\n",
    "\n",
    "for classes in CLASSES_TO_CLASSIFY:\n",
    "    \n",
    "    train_class_dir = join(train_dir, classes)\n",
    "    validation_class_dir = join(validation_dir, classes)\n",
    "    test_class_dir = join(test_dir, classes)\n",
    "    \n",
    "    # Make train sub directories\n",
    "    make_dir_helper(train_class_dir)\n",
    "    make_dir_helper(validation_class_dir)\n",
    "    make_dir_helper(test_class_dir)\n",
    "    \n",
    "    \n",
    "    original_dataset_class_dir = join(original_dataset_dir, classes)\n",
    "    fnames = [fname for fname in listdir(original_dataset_class_dir) if isfile(join(original_dataset_class_dir,fname)) and is_jpeg(fname)]\n",
    "    \n",
    "    no_of_samples = len(fnames)\n",
    "    shuffle(fnames)\n",
    "    \n",
    "    no_of_samples_cut = round(no_of_samples * 0.7)\n",
    "    \n",
    "    for idx, fname in enumerate(fnames):\n",
    "        \n",
    "        src_dir = join(original_dataset_class_dir, fname)\n",
    "        if idx < no_of_samples_cut:\n",
    "            dst_dir = join(train_class_dir, fname)\n",
    "            copyfile(src_dir, dst_dir)\n",
    "        elif no_of_samples_cut <= idx < no_of_samples:\n",
    "            dst_dir = join(validation_class_dir, fname)\n",
    "            copyfile(src_dir, dst_dir)\n",
    "            \n",
    "            dst_dir = join(test_class_dir, fname)\n",
    "            copyfile(src_dir, dst_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the CNN Model\n",
    "Design a CNN model for multi-label classification. The model is designed on top the original VGG 16 dataset which produces the most accurate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO saved model with name dr_model.h5\n",
      "Found 8330 images belonging to 5 classes.\n",
      "Found 6393 images belonging to 5 classes.\n",
      "data batch: (15, 128, 128, 3)\n",
      "labels batch: (15, 5)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 16,813,381\n",
      "Trainable params: 2,098,693\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      " 9/10 [==========================>...] - ETA: 4s - loss: 1.6266 - acc: 0.6296"
     ]
    }
   ],
   "source": [
    "def compile_model(model):\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "             metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "model = None\n",
    "if exists(model_file_path):\n",
    "    model = load_model(model_file_path)\n",
    "    print('Found saved model with name ' + MODEL_FILE_NAME)\n",
    "    compile_model(model)\n",
    "\n",
    "else:\n",
    "    print('NO saved model with name ' + MODEL_FILE_NAME)   \n",
    "    train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                       vertical_flip=True,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       horizontal_flip=True)  \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "            train_dir,  \n",
    "            target_size=(IMAGE_SIZE, IMAGE_SIZE),  \n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical')  \n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "            validation_dir,\n",
    "            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical')\n",
    "\n",
    "    for idx, (data_batch, labels_batch) in enumerate(train_generator):\n",
    "        print('data batch:', data_batch.shape)\n",
    "        print('labels batch:', labels_batch.shape)\n",
    "        break\n",
    "\n",
    "    model = configure_model()\n",
    "\n",
    "    compile_model(model)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='acc', patience=3, mode='auto'),\n",
    "        ModelCheckpoint(monitor='val_loss', save_best_only=True, filepath=model_file_path)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=10,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks = callbacks,\n",
    "        validation_steps=10)\n",
    "\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'b+')\n",
    "    plt.plot(epochs, val_acc, 'bo')\n",
    "    plt.title('Training and validation accuracy')\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss, 'b+')\n",
    "    plt.plot(epochs, val_loss, 'bo')\n",
    "    plt.title('Training and validation loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model \n",
    "Evalute the model using a confusion matrix to understand the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_class in CLASSES_TO_CLASSIFY: \n",
    "    \n",
    "    dir_name = join(test_dir, curr_class);\n",
    "    no_of_samples = len(listdir(dir_name))\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "            test_dir,\n",
    "            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            classes = [curr_class],\n",
    "            shuffle=False,\n",
    "            save_format='jpeg',\n",
    "            class_mode=None)\n",
    "    predictions = model.predict_generator(test_generator, (no_of_samples // BATCH_SIZE) + 1 , max_queue_size=10, workers=8, use_multiprocessing=False, verbose=0)\n",
    "   \n",
    "    correct_count = 0\n",
    "    \n",
    "    print(predictions.shape)\n",
    "    \n",
    "    for idx, prediction in enumerate(predictions):\n",
    "        \n",
    "        max_val = np.amax(prediction)\n",
    "        max_index = np.argmax(prediction)\n",
    "        pred_class = CLASSES_TO_CLASSIFY[max_index]\n",
    "        \n",
    "        if curr_class == pred_class:\n",
    "            correct_count += 1\n",
    "      \n",
    "    print(curr_class + ':' + str(correct_count / len(predictions) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
